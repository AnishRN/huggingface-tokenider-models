{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD1_ImsH77f4"
   },
   "source": [
    "# 🧠Batch Training of Subword Tokenizers for Medium-Grade LLMs on WikiText-103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aAgiqqE77iq"
   },
   "source": [
    "# 📌 Problem Statement\n",
    "Training an effective **tokenizer** is a critical step in building **medium-grade language models (100M–350M parameters)**. Tokenizers convert raw text into subword units that the model can understand, and the choice of tokenizer affects:\n",
    "\n",
    "- Vocabulary coverage and out-of-vocabulary (OOV) handling  \n",
    "- Sequence length efficiency  \n",
    "- Downstream model performance on generation, summarization, and understanding tasks  \n",
    "\n",
    "This project aims to **train and compare a batch of tokenizers**—**BPE, WordPiece, Unigram, and optional Char-level**—on the **WikiText-103 dataset**, each with a **vocabulary size of ~60,000 tokens**. By training multiple tokenizer types on the same corpus, we can evaluate which approach best balances **coverage, efficiency, and adaptability** for medium-grade LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KC47t4I854G"
   },
   "source": [
    "# 📂 Dataset Overview\n",
    "\n",
    "**Dataset Name:** WikiText-103 (raw version)  \n",
    "**Source:** [Hugging Face - WikiText](https://huggingface.co/datasets/wikitext)  \n",
    "\n",
    "**Description:**  \n",
    "WikiText-103 is a large-scale English language modeling dataset extracted from verified Good and Featured articles on Wikipedia. It contains over **100 million tokens** and provides high-quality, clean text suitable for training tokenizers and language models.  \n",
    "\n",
    "**Split Used:**  \n",
    "- **Train Split:** `train` (streaming mode used for memory efficiency)  \n",
    "\n",
    "**Key Features:**  \n",
    "- **Text-based**: Contains full articles with proper formatting.  \n",
    "- **High-quality language**: Extracted from curated Wikipedia articles.  \n",
    "- **Size:** ~103 million tokens  \n",
    "- **Use Case:** Ideal for training subword tokenizers and general-purpose medium-grade LLMs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aapUnHEm-CxC"
   },
   "source": [
    "# 🔹 Tokenizer Types and References\n",
    "\n",
    "In this project, we will train and compare **multiple types of tokenizers** on the WikiText-103 dataset. Each tokenizer has different properties and advantages for language modeling:\n",
    "\n",
    "1. **BPE (Byte Pair Encoding)**  \n",
    "   - Subword-level tokenizer that merges frequent character pairs.  \n",
    "   - Efficient for handling rare words while keeping vocabulary compact.  \n",
    "   - Widely used in GPT-style models.  \n",
    "   - **References:**  \n",
    "     - [Original Paper: Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)  \n",
    "     - [Hugging Face BPE Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/components.html#byte-level-bpe)\n",
    "\n",
    "2. **WordPiece**  \n",
    "   - Builds subwords based on maximizing likelihood of the training data.  \n",
    "   - Used in BERT and similar Transformer models.  \n",
    "   - Balances vocabulary size and coverage efficiently.  \n",
    "   - **References:**  \n",
    "     - [Original Paper: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)  \n",
    "     - [Hugging Face WordPiece Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/components.html#wordpiece)\n",
    "\n",
    "3. **Unigram (SentencePiece)**  \n",
    "   - Probabilistic tokenizer that selects subwords to maximize corpus likelihood.  \n",
    "   - Flexible and often achieves better tokenization on diverse text.  \n",
    "   - **References:**  \n",
    "     - [Original Paper: SentencePiece: A simple and language independent subword tokenizer and detokenizer](https://arxiv.org/abs/1808.06226)  \n",
    "     - [Hugging Face Unigram Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/components.html#unigram)\n",
    "\n",
    "**General Documentation & Resources:**  \n",
    "- [Hugging Face Tokenizers Documentation](https://huggingface.co/docs/tokenizers/index)  \n",
    "- [WikiText-103 Dataset on Hugging Face](https://huggingface.co/datasets/wikitext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apV0i5GF-jbL"
   },
   "source": [
    "---\n",
    "---\n",
    "# 🛠️ Project Build\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIyGqTj604UX"
   },
   "source": [
    "---\n",
    "##BPE (Byte Pair Encoding) Tokeniser\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HwgpYSn_nND"
   },
   "source": [
    "### 🔹 Phase 1 : Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQsrpVUZAHlK"
   },
   "source": [
    "#### 🛠 Dependencies\n",
    "\n",
    "The project requires the following packages and tools:\n",
    "\n",
    "- **datasets**: For loading and streaming the WikiText-103 dataset efficiently.  \n",
    "- **evaluate**: Provides metrics for evaluating tokenizer and model performance.  \n",
    "- **transformers with SentencePiece support**: Needed for training Unigram and other tokenizers.  \n",
    "- **git-lfs**: Handles large files when working with Hugging Face models or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVAopA0q_kjk",
    "outputId": "f277d381-319d-4ade-87d0-3dfa2828643b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.6.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut3ySS25AKCb"
   },
   "source": [
    "#### 📚 Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tyv9_ih6ARdb"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers import Tokenizer, models, normalizers, Regex\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents, Replace, Strip\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fihysVIkAtBr"
   },
   "source": [
    "#### 🔑 Hugging Face API Key and Login Setup\n",
    "\n",
    "To access datasets and models from Hugging Face, the following steps are required:\n",
    "\n",
    "- Retrieve the **Hugging Face API token** from user data or environment variables.  \n",
    "- Use the API token to **authenticate** with the Hugging Face Hub.  \n",
    "- Perform a **notebook login** to enable access to datasets and models directly within Google Colab.  \n",
    "\n",
    "This ensures that streaming datasets and model downloads are authorized and seamless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bzUSPXsR6Ic9"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HFToken')\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWpkegUEBBWS"
   },
   "source": [
    "### 🔹 Phase 2 : Corpus and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ1fmeAwBkAY"
   },
   "source": [
    "#### 📥 Data Loading\n",
    "\n",
    "- Loaded the **WikiText-103 (raw) dataset** from Hugging Face.  \n",
    "- Used the **train split** of the dataset for tokenizer training.  \n",
    "- Enabled **streaming mode** to efficiently handle the large dataset without consuming excessive memory.  \n",
    "- Verified the dataset by inspecting its samples to ensure proper loading.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251,
     "referenced_widgets": [
      "d3408a63a2b340f288204cdc971eba93",
      "6b940bf87e5f44bea87a4d2368c5cf97",
      "e37973f36d3540e0a6952243ef476510",
      "c095b80c7d2e4d5aa86b5914a1cc9af0",
      "8fc3430dea754558858f71cba29f8b75",
      "b0960510506240b4a40bbaa5e2bf3f71",
      "f56911a377ca4427adca37fc5fb8e13e",
      "d303dabe062545f9b86c7223e8b8e67f",
      "286f2c9401014abb8142068034a143cd",
      "4527a7f4b07a464181a44c2395ca4985",
      "4c28ee5ff6624ceda8adc0d1b3551319"
     ]
    },
    "id": "BK5a6XHH_TVb",
    "outputId": "5a1c2de6-2037-412a-eff7-101ed826f537"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3408a63a2b340f288204cdc971eba93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: ['text'],\n",
       "    num_shards: 2\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\", streaming=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzzTJggCBt17"
   },
   "source": [
    "#### 🔄 Iterator / Training Corpus Configuration\n",
    "\n",
    "- Defined a **batch iterator** to process the dataset in manageable chunks.  \n",
    "- Each batch contains **1000 text samples** by default.  \n",
    "- The iterator **yields batches of text** for efficient tokenizer training.  \n",
    "- Ensures that the entire dataset can be streamed without loading it fully into memory.  \n",
    "- Handles any remaining samples at the end of the dataset to avoid data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8tqW1-j7BOCS"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus(batch_size=1000):\n",
    "    batch = []\n",
    "    for example in dataset:\n",
    "        batch.append(example[\"text\"])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3-qbotsCDer"
   },
   "source": [
    "###🔹 Phase 3 : Tokeniser Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4DESgHxCB_Z"
   },
   "source": [
    "#### ⚙️ Tokenizer Initialization and Pre-tokenization\n",
    "\n",
    "1. **Tokenizer Initialization**  \n",
    "   - Created a **BPE (Byte Pair Encoding) tokenizer** instance.  \n",
    "   - This defines the base model that will learn subword units from the dataset.  \n",
    "\n",
    "2. **Pre-tokenizer Configuration**  \n",
    "   - Set the **pre-tokenizer** to `ByteLevel`, which splits text into bytes while preserving whitespace.  \n",
    "   - Pre-tokenization prepares the raw text into smaller units before applying BPE merges.  \n",
    "\n",
    "3. **Pre-tokenization Test**  \n",
    "   - Tested the pre-tokenizer with a sample string `\"Let's test pre-tokenization!\"`.  \n",
    "   - Ensures that text is correctly split into tokens, validating the pre-tokenizer setup before training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EclYxgGR_TYM",
    "outputId": "db27fc1a-4fdc-4e3d-c8b8-147554f0449e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġpre', (10, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAlpUV8NCXIK"
   },
   "source": [
    "### 🔹 Phase 3 : Tokeniser Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zie7vdlrCVwC"
   },
   "source": [
    "#### 🏋️ Tokenizer Model Training and Config\n",
    "\n",
    "- Configured a **BPE trainer** with a **vocabulary size of 60,000 tokens**.  \n",
    "- Added **special tokens** (e.g., `<|endoftext|>`) required for downstream tasks.  \n",
    "- Used the **training corpus iterator** to feed the tokenizer with batches of text.  \n",
    "- Called the training function to **learn the subword vocabulary and merge rules** from the dataset.  \n",
    "- Ensured that the tokenizer is ready to encode and decode text efficiently for medium-grade LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-kP0Qmd_Taq"
   },
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=60000, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfXwdYWzC_To"
   },
   "source": [
    "###🔹 Phase 4  : Tokeniser Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aBSu7NbCjVL"
   },
   "source": [
    "#### 💾 Tokenizer Saving\n",
    "\n",
    "- **Saved the trained tokenizer** locally to the directory `./wrapped_tokenizer`.  \n",
    "- Ensures the tokenizer can be **reloaded easily** for future model training or inference.  \n",
    "- Provides a persistent version of the tokenizer for **reuse across projects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyhFTECw_TdO"
   },
   "outputs": [],
   "source": [
    "tokenizer.save(\"english_bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0CXM1KiDLPy"
   },
   "source": [
    "#### ☁️ Tokenizer Wrapping and Hub Upload\n",
    "\n",
    "- **Wrapped the trained tokenizer** using `PreTrainedTokenizerFast` to make it compatible with Hugging Face models.  \n",
    "- Assigned **special tokens** for unknown, padding, beginning-of-sequence, and end-of-sequence.  \n",
    "- **Pushed the tokenizer to the Hugging Face Hub**, enabling public access and version control.  \n",
    "- This allows the tokenizer to be **shared, reused, and integrated** into other projects or LLM training pipelines.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5a5hOs0_Tfx",
    "outputId": "24b31eb0-4f8c-48f0-ce8d-4a54ce2f9d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./wrapped_tokenizer/tokenizer_config.json',\n",
       " './wrapped_tokenizer/special_tokens_map.json',\n",
       " './wrapped_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"english_bpe_tokenizer.json\",\n",
    "    unk_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|endoftext|>\",\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\"\n",
    ")\n",
    "\n",
    "wrapped_tokenizer.save_pretrained(\"./wrapped_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhgqapeM_Tic"
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AC8JACnu_Tm0"
   },
   "outputs": [],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"english_bpe_tokenizer-60k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8K5vuSkF9py"
   },
   "source": [
    "---\n",
    "## Wordpiece Tokeniser\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5YhV5-uF9sB"
   },
   "source": [
    "### 🧩 WordPiece Tokenizer – Normalizer Definition\n",
    "\n",
    "- Since the dataset and iterator were already prepared in earlier steps, we directly moved to **defining the normalizer** for the WordPiece tokenizer.  \n",
    "- **Why a normalizer is required here:**  \n",
    "  - WordPiece relies heavily on consistent text preprocessing because it builds its vocabulary from **subword units** that are sensitive to character case, punctuation, and diacritics.  \n",
    "  - Inconsistent casing or hidden Unicode variations can fragment the vocabulary, reducing efficiency and increasing unknown tokens (`[UNK]`).  \n",
    "- **Why not needed for BPE-based tokenizer:**  \n",
    "  - BPE inherently merges character sequences based on frequency and can naturally adapt to small variations in casing or symbols.  \n",
    "  - While normalization can still be used with BPE, it is not strictly required for stable performance, unlike with WordPiece where normalization directly impacts vocabulary coherence and tokenization accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7tS4lx2P9t"
   },
   "source": [
    "###🔹 Phase 3 : Tokeniser Model Building\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WAXJJ4-3jIQ"
   },
   "source": [
    "#### 🔧 Normalization Step\n",
    "We applied a sequence of normalization operations to ensure clean, consistent input before tokenization:\n",
    "\n",
    "1. **NFD()** – Converts characters into their *decomposed Unicode form* (e.g., `é` → `e` + `´`).\n",
    "2. **Lowercase()** – Converts all characters to lowercase for case-insensitive processing.\n",
    "3. **StripAccents()** – Removes accent marks and diacritics from characters.\n",
    "4. **Replace(Regex(r\"[\\u0000-\\u001F\\u007F]\"), \" \")** – Replaces non-printable ASCII control characters with a space.\n",
    "5. **Replace(Regex(r\"\\s+\"), \" \")** – Collapses multiple consecutive spaces into a single space.\n",
    "6. **Strip()** – Removes any leading or trailing whitespace.\n",
    "\n",
    "**Result:**  \n",
    "Input: `\"Héllò hôw are ü?\"`  \n",
    "Output after normalization: `\"hello how are u?\"`  \n",
    "\n",
    "This ensures the WordPiece model works with **uniform, noise-free text**, reducing fragmentation of subword vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWoSdfhU2LL2",
    "outputId": "3c9f5bbd-9c9a-4dbf-9578-5f31bc602c0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    NFD(),\n",
    "    Lowercase(),\n",
    "    StripAccents(),\n",
    "    Replace(Regex(r\"[\\u0000-\\u001F\\u007F]\"), \" \"),\n",
    "    Replace(Regex(r\"\\s+\"), \" \"),\n",
    "    Strip()\n",
    "])\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEHXextn5Hw5"
   },
   "source": [
    "#### 🔧Pre-tokenization Step (WordPiece Tokenizer)\n",
    "\n",
    "For the WordPiece tokenizer, we directly moved to defining the **normalizer** since the dataset and iterator were already prepared. Unlike BPE, WordPiece benefits from a normalization stage because it is more sensitive to text inconsistencies (e.g., mixed cases, accented characters) that can inflate vocabulary size and reduce generalization.  \n",
    "\n",
    "We experimented with different pre-tokenization approaches to split the text into smaller, meaningful units before training:\n",
    "\n",
    "1. **Whitespace Tokenization** – Splits text wherever spaces appear.  \n",
    "   Example: `\"Let's test my pre-tokenizer.\"` → `[\"Let's\", \"test\", \"my\", \"pre-tokenizer.\"]`\n",
    "\n",
    "2. **Whitespace Split** – Similar to whitespace tokenization, but ensures cleaner handling of space-delimited words.\n",
    "\n",
    "3. **Sequence Pre-tokenization** – Combines multiple rules:  \n",
    "   - `WhitespaceSplit()` → Splits on spaces  \n",
    "   - `Punctuation()` → Separates punctuation marks from words  \n",
    "   - `Digits(individual_digits=False)` → Keeps multi-digit numbers together\n",
    "\n",
    "4. **ByteLevel Pre-tokenization** – Splits into byte-level tokens while preserving exact spacing and characters. Adds a `prefix space` for consistent handling of leading spaces.\n",
    "\n",
    "**Why Normalization Here but Not in BPE?**  \n",
    "BPE often learns merges that inherently account for raw text variability. WordPiece, however, relies on exact token matches; inconsistencies like `\"Hello\"` vs `\"hello\"` or `\"café\"` vs `\"cafe\"` would create redundant tokens. Normalization (lowercasing, NFD/strip accents, etc.) ensures uniform input, improving efficiency and vocabulary quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_suqv8bE3sEm",
    "outputId": "b46f97d5-19eb-421e-8059-deb84aaec734"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3Szg1KZ33u-",
    "outputId": "382fd5d8-9f33-47af-c465-518b164dddae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Let's\", (0, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre-tokenizer.', (14, 28))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGi8hIBF36I3",
    "outputId": "317b276c-ee74-43aa-cccd-5fb4b564d88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ĠLet', (0, 3)),\n",
       " (\"'s\", (3, 5)),\n",
       " ('Ġtest', (5, 10)),\n",
       " ('Ġmy', (10, 13)),\n",
       " ('Ġpre', (13, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(),\n",
    "    pre_tokenizers.Punctuation(),\n",
    "    pre_tokenizers.Digits(individual_digits=False)\n",
    "])\n",
    "pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBayx1Af680J"
   },
   "source": [
    "### 🔹 Phase 4 : Tokeniser Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6idygVns7OCl"
   },
   "source": [
    "#### 🏋️Model Setup — WordPiece Tokenizer Training\n",
    "\n",
    "We configure a **WordPieceTrainer** to train our tokenizer with the following parameters:\n",
    "\n",
    "- **`vocab_size=60000`** → Limits the vocabulary to the top 60,000 tokens (including special tokens).\n",
    "- **`special_tokens`** → Defines special-purpose tokens:\n",
    "  - `[UNK]` → Unknown token (for out-of-vocabulary words)\n",
    "  - `[PAD]` → Padding token (for sequence length alignment)\n",
    "  - `[CLS]` → Classification token (for sequence-level tasks)\n",
    "  - `[SEP]` → Separator token (e.g., between sentences)\n",
    "  - `[MASK]` → Masking token (for masked language modeling)\n",
    "- **`show_progress=True`** → Displays training progress.\n",
    "- **`min_frequency=2`** → Ignores tokens that appear fewer than 2 times in the dataset.\n",
    "- **`limit_alphabet=1000`** → Restricts the number of unique characters considered for tokenization.\n",
    "- **`continuing_subword_prefix=\"##\"`** → Prefix for subword tokens that continue from a previous word (e.g., `play` → `play`, `##ing`).\n",
    "\n",
    "This setup ensures a **balanced vocabulary** with efficient handling of rare words, subwords, and special-purpose tokens for downstream NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "53UwQKka5UfG"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=60000,\n",
    "    special_tokens=special_tokens,\n",
    "    show_progress=True,\n",
    "    min_frequency=2,\n",
    "    limit_alphabet=1000,\n",
    "    continuing_subword_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MinNLLYo7Xg3"
   },
   "source": [
    "#### 🏋️Training the WordPiece Tokenizer\n",
    "With the trainer configured, we now train the WordPiece tokenizer on our prepared dataset iterator.\n",
    "\n",
    "- **`train_from_iterator()`**  \n",
    "  This method consumes batches of text generated by our `get_training_corpus()` function and learns the vocabulary and subword units according to the WordPiece algorithm.\n",
    "\n",
    "- **Training flow**  \n",
    "  1. The pre-tokenizer splits the text into initial tokens (based on whitespace, punctuation, digits, etc.).  \n",
    "  2. The normalizer applies case folding or other preprocessing.  \n",
    "  3. The WordPiece algorithm iteratively builds the vocabulary starting with characters, merging the most frequent subword pairs until the target vocab size (`60000`) is reached.  \n",
    "  4. Special tokens are reserved and remain untouched during merges.\n",
    "\n",
    "- **Key outcome**  \n",
    "  After this phase, the tokenizer has a fully learned vocabulary and merge rules, enabling it to tokenize any input text into consistent WordPiece tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qHU_tKDW5Uhf"
   },
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "St8kGXPM5Uji",
    "outputId": "950e2627-f743-4a0a-91e6-5d5afde275c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['let', \"'\", 's', 'test', 'this', 'token', '##izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf3RDcYV9mxk"
   },
   "source": [
    "###🔹 Phase 5 : Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF1mmS8_9vif"
   },
   "source": [
    "#### 🔧 Post-Tokenization: Special Token Handling\n",
    "In this phase, the IDs of reserved special tokens such as `[CLS]` and `[SEP]` are retrieved from the tokenizer’s vocabulary.  \n",
    "This step ensures that these tokens are correctly recognized and mapped, which is crucial for downstream tasks like classification or sentence-pair processing where such tokens serve as sequence markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yuQ6B6Xv8WA4",
    "outputId": "6a6093f5-329a-488b-b45f-7cb83f72b314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZH6MruJ-UkO"
   },
   "source": [
    "#### 🔧 Post-Processing Template Setup\n",
    "Here, a post-processing template is defined to control how special tokens are inserted during tokenization.  \n",
    "The configuration specifies:\n",
    "- **Single sequence format**: `[CLS]` at the start, followed by the input tokens, ending with `[SEP]`.\n",
    "- **Pair sequence format**: `[CLS]` → first sequence → `[SEP]` → second sequence → `[SEP]`.\n",
    "- **Special tokens mapping**: `[CLS]` and `[SEP]` are explicitly linked to their IDs.  \n",
    "\n",
    "A test encoding is then run on both a single and paired input to verify:\n",
    "- Correct insertion of special tokens.\n",
    "- Proper assignment of **type IDs** (segment embeddings) to distinguish sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YGiNWM2E8WDJ",
    "outputId": "2767b046-3e86-4df3-9bd3-fc51adf9063e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'token', '##izer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMes21pZ-cUl"
   },
   "source": [
    "#### 🔧 Decoder Setup\n",
    "The decoder is configured to reverse the WordPiece tokenization process.  \n",
    "- **Prefix handling**: The `\"##\"` prefix (used in WordPiece to indicate a subword continuation) is removed during decoding.  \n",
    "- **Purpose**: Converts token IDs back into a readable text string by joining subwords seamlessly.  \n",
    "\n",
    "Finally, the encoded token IDs are decoded to reconstruct the original text, confirming that tokenization and detokenization are consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "hdwCwsfa9c7V",
    "outputId": "8c9c9302-3fc4-4c10-d3b6-cdf51f4f282d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"let ' s test this tokenizer... on a pair of sentences.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w-BWpoz-nou"
   },
   "source": [
    "###🔹 Phase 6 : Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfLXuOU8-2ZJ"
   },
   "source": [
    "####💾 Tokenizer Saving\n",
    "- **Saved the trained tokenizer** locally to the directory.  \n",
    "- Ensures the tokenizer can be **reloaded easily** for future model training or inference.  \n",
    "- Provides a persistent version of the tokenizer for **reuse across projects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwaQ0blI9c9m",
    "outputId": "103ca5b7-d7ff-4a8d-9bac-72e4109194e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./english-wordpiece-tokenizer-60k/tokenizer_config.json',\n",
       " './english-wordpiece-tokenizer-60k/special_tokens_map.json',\n",
       " './english-wordpiece-tokenizer-60k/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(\"english_wordpiece_tokenizer.json\")\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"english_wordpiece_tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "wrapped_tokenizer.save_pretrained(\"./english-wordpiece-tokenizer-60k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lixvyab3_-IV"
   },
   "source": [
    "#### ☁️ Hub Upload\n",
    "\n",
    "- **Wrapped the trained tokenizer** using `PreTrainedTokenizerFast` to make it compatible with Hugging Face models.  \n",
    "- Assigned **special tokens** for unknown, padding, beginning-of-sequence, and end-of-sequence.  \n",
    "- **Pushed the tokenizer to the Hugging Face Hub**, enabling public access and version control.  \n",
    "- This allows the tokenizer to be **shared, reused, and integrated** into other projects or LLM training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "2e3b7897393243ba9da7068942f28562",
      "e52a0854f4bc404b953b22875f4837b1",
      "9e3f1c48dece41dabb551a9be5130f16",
      "fa5aa354e7d94962958cb77b5b531e3d",
      "ded2c1ab97f44f9eb0d2f3d3d7433199",
      "02e9bb60422d4082a21998982c000b48",
      "f9f48fe7824b456481acfe6d2c2314f0",
      "762597a359fd4171a4e2abc29f63d6fe",
      "b22766da12e145d4a11365e413eafc42",
      "4988bb9394a948e08b9bd5667fc4ef54",
      "5486d8aa0c6e44b2b063a01c33700fea",
      "bffb457cdf924905bb821cf73748a9c7",
      "8772215bfde149c48c1cd021268a8aca",
      "89d70c8452984fef8f85c840fde95e9b",
      "385885081ace46f29cf9347face44013",
      "c8777460520949e7851e54c25f859db3",
      "ddbd45dd55ad42deaaf16895ad0e81c6",
      "e37b299debd34485806ae15fd681ed85",
      "437b175b9af34863821a053c3a173e71",
      "d7c95fdb70c7473fb30ff763a0aff28f"
     ]
    },
    "id": "Yl2iLgnMAJuV",
    "outputId": "8bcfc1a8-0f83-402e-8225-4c89ab973d62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3b7897393243ba9da7068942f28562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "iyFvv22IAO1c",
    "outputId": "4955fe90-d4c5-4470-b0c1-40fffb7b64b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yakul259/english-wordpiece-tokenizer-60k/commit/798b5f179cff3bd7bd075f617ce78c3450af1dee', commit_message='Upload tokenizer', commit_description='', oid='798b5f179cff3bd7bd075f617ce78c3450af1dee', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yakul259/english-wordpiece-tokenizer-60k', endpoint='https://huggingface.co', repo_type='model', repo_id='yakul259/english-wordpiece-tokenizer-60k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"english-wordpiece-tokenizer-60k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnc6DHkDFHAd"
   },
   "source": [
    "---\n",
    "## Unigram Tokeniser\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYtUOlxtFKSO"
   },
   "source": [
    "###🔹 Phase 3 : Tokeniser Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9g-hf1YnGaAq"
   },
   "source": [
    "#### 🔧 Normalization for Unigram Tokenizer\n",
    "\n",
    "Since we already have our dataset (`wikitext-103-raw-v1`) and the training corpus generator ready from previous phases, we will now proceed directly to **normalization** for our Unigram tokenizer.\n",
    "\n",
    "In this phase, we will:\n",
    "- Apply **NFD Unicode normalization** to ensure consistent character representation.\n",
    "- Strip accents from characters for uniformity.\n",
    "- Convert all text to lowercase to reduce vocabulary size.\n",
    "- Remove control characters that might interfere with tokenization.\n",
    "\n",
    "This normalization ensures that different visual forms of the same character are treated identically, improving the tokenizer’s generalization and reducing unnecessary vocabulary entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4o7-pGrKFz9Z"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTO6gL6dG09u"
   },
   "source": [
    "#### 🔧 Pre-tokenization Step (Unigram Tokenizer)\n",
    "After normalization, we move to **pre-tokenization**, which splits text into manageable segments before actual token learning.\n",
    "\n",
    "Here, we use the **Metaspace** pre-tokenizer:\n",
    "- Replaces spaces with a special visible character (default `_`), making spaces explicit in the tokenization process.\n",
    "- Ensures consistent handling of whitespace across the dataset.\n",
    "- Preserves original text boundaries while still allowing the tokenizer to operate on subword units.\n",
    "\n",
    "Example:\n",
    "- Input: `\"Let's test the pre-tokenizer!\"`\n",
    "- Output: Spaces become `_` and tokens are clearly separated, aiding in vocabulary learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djLhLrhHHHMe",
    "outputId": "68709833-5041-405e-b52a-df9bf0606b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"▁Let's\", (0, 5)),\n",
       " ('▁test', (5, 10)),\n",
       " ('▁the', (10, 14)),\n",
       " ('▁pre-tokenizer!', (14, 29))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgrJ9ZfuIr7V"
   },
   "source": [
    "###🔹 Phase 4 : Tokeniser Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rNk3yMyIxzB"
   },
   "source": [
    "#### 🏋️Model Setup — Unigram Tokenizer Config\n",
    "In this phase, we train the **Unigram Tokenizer** using our prepared corpus and configuration.\n",
    "\n",
    "- **Trainer Used:** `UnigramTrainer`\n",
    "- **Vocabulary Size:** `25,000` tokens — chosen to balance coverage and model size.\n",
    "- **Special Tokens:**  \n",
    "  - `<cls>` — Classification token (for sentence-level tasks).  \n",
    "  - `<sep>` — Separator token (for splitting sequences).  \n",
    "  - `<unk>` — Unknown token (for unseen words/subwords).  \n",
    "  - `<pad>` — Padding token (for batch processing).  \n",
    "  - `<mask>` — Masking token (for MLM tasks).  \n",
    "  - `<s>` and `</s>` — Start and end of sequence tokens.\n",
    "\n",
    "- **Unknown Token Handling:**  \n",
    "  Any text fragment not in the vocabulary will map to `<unk>`.\n",
    "\n",
    "- **Training Process:**  \n",
    "  Uses `train_from_iterator(get_training_corpus(), trainer=trainer)` — iteratively feeds batches of text to the trainer for subword vocabulary optimization.\n",
    "\n",
    "- **Goal:**  \n",
    "  Build a compact and efficient subword vocabulary that captures the most probable token combinations based on frequency and likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "upga-rB3Har2"
   },
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=60000,\n",
    "    special_tokens=special_tokens,\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywWeVQA4JNPM"
   },
   "source": [
    "####🏋️Model Setup — Unigram Tokenizer Training\n",
    "\n",
    "In this step, we explicitly initialize the **Unigram model** inside our tokenizer and begin the training process.\n",
    "\n",
    "- **Model Initialization:**  \n",
    "  `tokenizer.model = models.Unigram()`  \n",
    "  This sets the tokenizer's core algorithm to **Unigram**, a probabilistic subword segmentation method that selects the most likely tokenization of text based on learned frequencies.\n",
    "\n",
    "- **Training Execution:**  \n",
    "  `tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)`  \n",
    "  - **`get_training_corpus()`** → Streams batches of raw text from our dataset to avoid memory overload.\n",
    "  - **`trainer`** → Contains vocabulary size, special tokens, and unknown token settings.\n",
    "  - The training process evaluates token candidates and iteratively removes the least probable ones until the vocabulary reaches the target size (25,000 in our case).\n",
    "\n",
    "- **Why Unigram?**  \n",
    "  - Produces a **probabilistic** tokenization, allowing multiple valid segmentations.\n",
    "  - Robust for noisy, multi-domain text data.\n",
    "  - Maintains better coverage for rare words compared to purely greedy approaches.\n",
    "\n",
    "- **Outcome:**  \n",
    "  A fully trained **Unigram Tokenizer model** with:\n",
    "  - Optimized vocabulary.\n",
    "  - Special token handling.\n",
    "  - Ready for encoding and decoding text in downstream tasks.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "N0Ag-pHpHau1"
   },
   "outputs": [],
   "source": [
    "tokenizer.model = models.Unigram()\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHJou-cuHaxG",
    "outputId": "4c61d687-a855-432b-aaeb-71a53276b3e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁token', 'izer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qTvhw5rJaPw"
   },
   "source": [
    "###🔹 Phase 5 : Post Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF2BeJAEJ1Ro"
   },
   "source": [
    "#### 🔧 Post-Tokenization: Special Token Handling\n",
    "In this phase, the IDs of reserved special tokens such as `[CLS]` and `[SEP]` are retrieved from the tokenizer’s vocabulary.  \n",
    "This step ensures that these tokens are correctly recognized and mapped, which is crucial for downstream tasks like classification or sentence-pair processing where such tokens serve as sequence markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSROe5LDHqJG",
    "outputId": "b59e9d73-647c-4968-c850-bf472429f272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJgBP8vgJ-Cg"
   },
   "source": [
    "#### 🔧 Post-Processing Template Setup\n",
    "Here, a post-processing template is defined to control how special tokens are inserted during tokenization.  \n",
    "The configuration specifies:\n",
    "- **Single sequence format**: `[CLS]` at the start, followed by the input tokens, ending with `[SEP]`.\n",
    "- **Pair sequence format**: `[CLS]` → first sequence → `[SEP]` → second sequence → `[SEP]`.\n",
    "- **Special tokens mapping**: `[CLS]` and `[SEP]` are explicitly linked to their IDs.  \n",
    "\n",
    "A test encoding is then run on both a single and paired input to verify:\n",
    "- Correct insertion of special tokens.\n",
    "- Proper assignment of **type IDs** (segment embeddings) to distinguish sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbHRZMy0HqMC",
    "outputId": "6d72e5e4-cdb9-4023-db67-08197c23fda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Let', \"'\", 's', '▁test', '▁this', '▁token', 'izer', '.', '.', '.', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!', '<sep>', '<cls>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n",
    ")\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks2r0enBK9k2"
   },
   "source": [
    "#### 🔧 Decoder Setup\n",
    "The decoder is configured to reverse the WordPiece tokenization process.  \n",
    "- **Purpose**: Converts token IDs back into a readable text string by joining subwords seamlessly.  \n",
    "\n",
    "Finally, the encoded token IDs are decoded to reconstruct the original text, confirming that tokenization and detokenization are consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3UpdgunBHqOo"
   },
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLjp3rBOLDbA"
   },
   "source": [
    "###🔹 Phase 6 : Model Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at-sbPzwLISo"
   },
   "source": [
    "####💾 Tokenizer Saving\n",
    "- **Saved the trained tokenizer** locally to a directory.\n",
    "- Ensures the tokenizer can be **reloaded easily** for future model training or inference.  \n",
    "- Provides a persistent version of the tokenizer for **reuse across projects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2SNqjVvH07t",
    "outputId": "c532e3c3-0c5a-4c0c-acb8-626431da4e1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./english-unigram-tokenizer-60k/tokenizer_config.json',\n",
       " './english-unigram-tokenizer-60k/special_tokens_map.json',\n",
       " './english-unigram-tokenizer-60k/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(\"english_unigram_tokenizer.json\")\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file= \"english_unigram_tokenizer.json\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "wrapped_tokenizer.save_pretrained(\"./english-unigram-tokenizer-60k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_To37bC-LYRI"
   },
   "source": [
    "#### ☁️ Hub Upload\n",
    "\n",
    "- **Wrapped the trained tokenizer** using `PreTrainedTokenizerFast` to make it compatible with Hugging Face models.  \n",
    "- Assigned **special tokens** for unknown, padding, beginning-of-sequence, and end-of-sequence.  \n",
    "- **Pushed the tokenizer to the Hugging Face Hub**, enabling public access and version control.  \n",
    "- This allows the tokenizer to be **shared, reused, and integrated** into other projects or LLM training pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "YXqEJmbkH4pI",
    "outputId": "56f5ebf3-25cc-4869-ef0b-06b2b617eefc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/yakul259/english-unigram-tokenizer-60k/commit/66f76b305f9b8af5f6af498e2d8047b7ba7d489b', commit_message='Upload tokenizer', commit_description='', oid='66f76b305f9b8af5f6af498e2d8047b7ba7d489b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/yakul259/english-unigram-tokenizer-60k', endpoint='https://huggingface.co', repo_type='model', repo_id='yakul259/english-unigram-tokenizer-60k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"english-unigram-tokenizer-60k\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
